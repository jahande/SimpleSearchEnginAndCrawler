Binary classification - Wikipedia, the free encyclopedia Binary classification From Wikipedia, the free encyclopedia Jump to: navigation, search Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a Classification rule. Some typical binary classification tasks are medical testing to determine if a patient has certain disease or not (the classification property is the presence of the disease) quality control in factories; i.e. deciding if a new product is good enough to be sold, or if it should be discarded (the classification property is being good enough) deciding whether a page or an article should be in the result set of a search or not (the classification property is the relevance of the article, or the usefulness to the user) Statistical classification in general is one of the problems studied in computer science, in order to automatically learn classification systems; some methods suitable for learning binary classifiers include the decision trees, Bayesian networks, support vector machines, neural networks, probit regression, and logit regression. Sometimes, classification tasks are trivial. Given 100 balls, some of them red and some blue, a human with normal color vision can easily separate them into red ones and blue ones. However, some tasks, like those in practical medicine, and those interesting from the computer science point-of-view, are far from trivial, and may produce faulty results if executed imprecisely. Contents 1 Evaluation of binary classifiers 1.1 Example 2 Converting continuous values to binary 3 See also 4 References 5 Bibliography Evaluation of binary classifiers[edit] Terminology and derivations from a confusion matrix true positive (TP) eqv. with hit true negative (TN) eqv. with correct rejection false positive (FP) eqv. with false alarm, Type I error false negative (FN) eqv. with miss, Type II error sensitivity or true positive rate (TPR) eqv. with hit rate, recall specificity (SPC) or True Negative Rate precision or positive predictive value (PPV) negative predictive value (NPV) fall-out or false positive rate (FPR) false discovery rate (FDR) Miss Rate or False Negative Rate (FNR) accuracy (ACC) F1 score is the harmonic mean of precision and sensitivity Matthews correlation coefficient (MCC) Informedness = Sensitivity + Specificity - 1 Markedness = Precision + NPV - 1 Source: Fawcett (2006). From the confusion matrix you can derive four basic measures To measure the performance of a classifier or predictor there are several values that can be used. Different fields have preferences for specific metric due to the known biases that are accepted. For example, in medicine the concepts sensitivity and specificity are often used. Say we test some people for the presence of a disease. Some of these people have the disease, and our test says they are positive. They are called true positives (TP). Some have the disease, but the test claims they don't. They are called false negatives (FN). Some don't have the disease, and the test says they don't - true negatives (TN). Finally, there might be healthy people who have a positive test result - false positives (FP). Thus, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Let us define an experiment from P positive instances and N negative instances for some known condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows: Condition (as determined by "Gold standard") Condition positive Condition negative Test outcome Test outcome positive True positive False positive (Type I error) Precision = ? True positive ? Test outcome positive Test outcome negative False negative (Type II error) True negative Negative predictive value = ? True negative ? Test outcome negative Sensitivity = ? True positive ? Condition positive Specificity = ? True negative ? Condition negative Accuracy Specificity (TNR) is the proportion of people that tested negative (TN) of all the people that actually are negative (TN+FP). As with sensitivity, it can be looked at as the probability that the test result is negative given that the patient is not sick. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, the less money the factory loses by discarding good products instead of selling them). Sensitivity (TPR), also known as recall, is the proportion of people that tested positive (TP) of all the people that actually are positive (TP+FN). It can be seen as the probability that the test is positive given that the patient is sick. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, the fewer faulty products go to the market). The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the ROC curve. In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator. Modern pregnancy tests do not use the pregnancy itself to determine pregnancy status; rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a surrogate marker to indicate that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100% (in that false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100% (in that false negatives are possible). In addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question "If the test result is positive, how well does that predict an actual presence of disease?". It is calculated as (true positives) / (true positives + false positives); that is, it is the proportion of true positives out of all positive results. (The negative prediction value is the same, but for negatives, naturally.) accuracy measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications. The F1 score is a measure of a test's performance when a single value is wanted. It considers both the precision and the recall of the test to compute the score. The traditional or balanced F-score is the harmonic mean of precision and recall: . Note, however, that the F-scores do not take the true negative rate into account, and that measures such as the Phi coefficient, Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.[1] As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (deltap').[2] Example[edit] As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested, 1000 of them are sick and 1000 of them are healthy. About 990 true positives 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result. However, if of the 2000 people only 100 are really sick: the likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease - that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that he or she really has the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result. Converting continuous values to binary[edit] Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff. However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as "positive" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as "positive" as the one of 52 mIU/ml. See also[edit] Multiclass classification Multi-label classification One-class classification Kernel methods Thresholding (image processing) Prosecutor's fallacy Examples of Bayesian inference Receiver operating characteristic Matthews correlation coefficient Classification rule Detection theory References[edit] This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2011) ^ Powers, David M W (2007/2011). "Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation". Journal of Machine Learning Technologies 2 (1): 37–63.  Check date values in: |date= (help) ^ Perruchet, P.; Peereman, R. (2004). "The exploitation of distributional information in syllable processing". J. Neurolinguistics 17: 97?119.  Bibliography[edit] Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book) John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 ([2] Kernel Methods Book) Bernhard Sch?lkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, MA, 2002. (Partly available on line: [3].) ISBN 0-262-19475-9 Statistics portal v t e Statistics   Descriptive statistics Continuous data Location Mean (Arithmetic, Geometric, Harmonic) Median Mode Dispersion Range Standard deviation Coefficient of variation Percentile Interquartile range Shape Variance Skewness Kurtosis Moments L-moments Count data Index of dispersion Summary tables Grouped data Frequency distribution Contingency table Dependence Pearson product-moment correlation Rank correlation (Spearman's rho, Kendall's tau) Partial correlation Scatter plot Statistical graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Run chart Scatter plot Stemplot Radar chart   Data collection Designing studies Effect size Standard error Statistical power Sample size determination Survey methodology Sampling Stratified sampling Opinion poll Questionnaire Controlled experiment Design of experiments Randomized experiment Random assignment Replication Blocking Factorial experiment Optimal design Uncontrolled studies Natural experiment Quasi-experiment Observational study   Statistical inference Statistical theory Sampling distribution Order statistic Scan statistic Sufficiency Completeness Exponential family Permutation test (Randomization test) Empirical distribution Bootstrap U statistic Efficiency Asymptotics Robustness Frequentist inference Unbiased estimator (Mean unbiased minimum variance, Median unbiased) Biased estimators (Maximum likelihood, Method of moments, Minimum distance, Density estimation) Confidence interval Testing hypotheses Power Parametric tests (Likelihood-ratio, Wald, Score) Specific tests Z (normal) Student's t-test F Goodness of fit (Chi-squared, G, Sample source, sample normality, Skewness & kurtosis Normality, Model comparison, Model quality) Signed-rank (1-sample, 2-sample, 1-way anova) Shapiro–Wilk Kolmogorov–Smirnov Bayesian inference Bayesian probability Prior Posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator   Correlation and regression analysis Correlation Pearson product–moment correlation Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression model validation Mixed effects models Simultaneous equations models MARS Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Heteroscedasticity Homoscedasticity Generalized linear model Exponential families Logistic (Bernoulli) Binomial Poisson Partition of variance Analysis of variance (ANOVA) Analysis of covariance Multivariate ANOVA Degrees of freedom   Categorical, multivariate, time-series, or survival analysis Categorical data Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Multivariate statistics Multivariate regression Principal components Factor analysis Cluster analysis Classification Copulas Time series analysis General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Specific tests Granger causality Q-Statistic Durbin–Watson Time domain ACF PACF XCF ARMA model ARIMA model ARCH Vector autoregression Frequency domain Spectral density estimation Fourier analysis Survival analysis Survival function Kaplan–Meier Logrank test Failure rate Proportional hazards models Accelerated failure time model   Applications Biostatistics Bioinformatics Clinical trials & studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process & Quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics National accounts Official statistics Population Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Portal Outline Index Retrieved from "http://en.wikipedia.org/w/index.php?title=Binary_classification&oldid=587607079" Categories: Statistical classification Machine learning Hidden categories: CS1 errors: dates Articles needing additional references from March 2011 All articles needing additional references Navigation menu Personal tools Create account Log in Namespaces Article Talk Variants Views Read Edit View history Actions Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Cite this page Print/export Create a book Download as PDF Printable version Languages Deutsch ÝÇÑÓ? ????? ??? Ti?ng Vi?t This page was last modified on 25 December 2013 at 06:58. Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Mobile view